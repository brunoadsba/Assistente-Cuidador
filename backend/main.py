from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, Response
from pydantic import BaseModel
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
import re
import os
import edge_tts
import uuid
import json
import traceback
from datetime import datetime
from dotenv import load_dotenv
from groq import Groq

# Carregar variÃ¡veis de ambiente
load_dotenv()

# Definir diretÃ³rio base do backend (onde main.py estÃ¡ localizado)
BACKEND_DIR = os.path.dirname(os.path.abspath(__file__))
print(f"ğŸ“‚ DiretÃ³rio do backend: {BACKEND_DIR}")

app = FastAPI()

# Ler chave do arquivo .env
GROQ_KEY = os.getenv("GROQ_API_KEY")
if not GROQ_KEY:
    raise ValueError("A chave GROQ_API_KEY nÃ£o foi encontrada no arquivo .env")

# Inicializar cliente Groq para Whisper (STT)
groq_client = Groq(api_key=GROQ_KEY)

# ConfiguraÃ§Ã£o CORS (Permite acesso do Frontend local)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- CONFIGURAÃ‡ÃƒO RAG & IA ---
# Usando Groq para LLM (GeraÃ§Ã£o RÃ¡pida) e Hugging Face para Embeddings (Online)
MODEL_NAME = "llama-3.3-70b-versatile"  # Modelo atualizado Groq
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"  # Modelo multilÃ­ngue profissional

print(f"ğŸ”„ Inicializando IA com Groq ({MODEL_NAME})...")

# 1. Carregar Manual Completo
MANUAL_FILE = "manual_cuidador.txt"

print(f"ğŸ“š Carregando manual completo: {MANUAL_FILE}")

# Cache do FAISS para evitar recalcular embeddings toda vez
CACHE_NAME = "manual_cuidador"
FAISS_CACHE_DIR = os.path.join(BACKEND_DIR, ".faiss_cache", CACHE_NAME)

embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL,
    model_kwargs={'device': 'cpu'},  # CPU Ã© suficiente para embeddings
    encode_kwargs={'normalize_embeddings': True}  # Melhora qualidade
)
vectorstore = None

# Tentar carregar do cache
if os.path.exists(FAISS_CACHE_DIR):
    try:
        print("âš¡ Carregando FAISS do cache (rÃ¡pido)...")
        vectorstore = FAISS.load_local(FAISS_CACHE_DIR, embeddings, allow_dangerous_deserialization=True)
        print("âœ… Cache carregado com sucesso!")
    except Exception as e:
        print(f"âš ï¸ Erro ao carregar cache: {e}. Recriando...")
        vectorstore = None

# Se nÃ£o tem cache, criar novo
if vectorstore is None:
    print("ğŸ”„ Criando novo FAISS (pode demorar alguns segundos)...")
    loader = TextLoader(MANUAL_FILE)
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(documents)
    
    # Criar Vector Store (FAISS)
    vectorstore = FAISS.from_documents(chunks, embeddings)
    
    # Salvar no cache para prÃ³xima vez
    try:
        os.makedirs(FAISS_CACHE_DIR, exist_ok=True)
        vectorstore.save_local(FAISS_CACHE_DIR)
        print("ğŸ’¾ Cache salvo para prÃ³xima inicializaÃ§Ã£o!")
    except Exception as e:
        print(f"âš ï¸ Erro ao salvar cache: {e}")

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 3. Setup LLM e Prompt
llm = ChatGroq(model=MODEL_NAME, temperature=0.3, api_key=GROQ_KEY)

# FunÃ§Ã£o para carregar histÃ³rico recente da conversa
def get_recent_history(limit=5):
    """Carrega as Ãºltimas N interaÃ§Ãµes do histÃ³rico"""
    try:
        if os.path.exists(LOG_FILE):
            with open(LOG_FILE, "r", encoding="utf-8") as f:
                history = json.load(f)
            # Retornar apenas as Ãºltimas N interaÃ§Ãµes
            recent = history[-limit:] if len(history) > limit else history
            return recent
    except Exception as e:
        print(f"âš ï¸ Erro ao carregar histÃ³rico: {e}")
    return []

# Prompt Otimizado com HistÃ³rico e Conhecimento Geral
template = """VocÃª Ã© um assistente especialista em cuidados com demÃªncia.

DIRETRIZES PRINCIPAIS:
1. SEJA SEMPRE DIRETO: Evite frases como "Entendo sua preocupaÃ§Ã£o" ou "nÃ£o hÃ¡ informaÃ§Ãµes no contexto fornecido". VÃ¡ direto ao ponto com informaÃ§Ãµes Ãºteis.
2. USE CONHECIMENTO GERAL: Se o contexto abaixo nÃ£o tiver a resposta completa, use seu conhecimento geral sobre cuidados geriÃ¡tricos e demÃªncia para ajudar. NUNCA diga que nÃ£o sabe - sempre forneÃ§a uma resposta Ãºtil.
3. MANTENHA CONTEXTO: Considere o histÃ³rico da conversa abaixo para evitar repetiÃ§Ãµes e criar respostas adequadas ao que jÃ¡ foi discutido.
4. FORMATO: Use listas e tÃ³picos sempre que possÃ­vel para facilitar a leitura.
5. IMPORTANTE: NÃƒO inclua cabeÃ§alhos como "Resposta Direta:", "Dicas:", "InformaÃ§Ãµes:" ou similares. Responda diretamente, sem rÃ³tulos ou tÃ­tulos.

HistÃ³rico da Conversa (Ãºltimas interaÃ§Ãµes):
{history}

Contexto da Base de Conhecimento:
{context}

Pergunta Atual do UsuÃ¡rio: {question}

Responda diretamente, sem cabeÃ§alhos ou rÃ³tulos:"""
prompt = ChatPromptTemplate.from_template(template)

# Criar chain que inclui histÃ³rico
def format_history(x):
    """Formata o histÃ³rico recente para o prompt"""
    recent_history = get_recent_history(limit=5)
    
    if recent_history:
        history_lines = []
        for entry in recent_history:
            history_lines.append(f"UsuÃ¡rio: {entry.get('user', '')}")
            history_lines.append(f"Assistente: {entry.get('assistant', '')}")
        return "\n".join(history_lines)
    return "Nenhuma conversa anterior."

# Chain com histÃ³rico
chain = (
    {
        "context": retriever,
        "history": RunnableLambda(format_history),
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | StrOutputParser()
)

# --- GUARDRAILS (REGRAS RÃGIDAS) ---
FORBIDDEN_TERMS = [
    r"medica[cÃ§][aÃ£]o", r"rem[eÃ©]dio", r"dose", r"posologia", 
    r"sangue", r"desmai", r"emerg[eÃª]ncia", r"samu", r"hospital"
]

def check_guardrails(text: str) -> bool:
    for pattern in FORBIDDEN_TERMS:
        if re.search(pattern, text, re.IGNORECASE):
            return False
    return True

def clean_text_for_tts(text: str) -> str:
    """Remove caracteres de markdown para leitura fluida"""
    text = re.sub(r'[*#\-]', '', text)
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_response_text(text: str) -> str:
    """Remove cabeÃ§alhos indesejados das respostas do LLM"""
    # Remover linhas que comeÃ§am com "Resposta Direta:", "Dicas:", "InformaÃ§Ãµes:", etc.
    lines = text.split('\n')
    cleaned_lines = []
    skip_next_empty = False
    
    for line in lines:
        line_stripped = line.strip()
        # Verificar se Ã© um cabeÃ§alho indesejado
        if re.match(r'^(Resposta Direta|Dicas|InformaÃ§Ãµes|OrientaÃ§Ãµes|SoluÃ§Ã£o|Resposta):\s*$', line_stripped, re.IGNORECASE):
            skip_next_empty = True
            continue
        
        # Pular linha vazia apÃ³s cabeÃ§alho removido
        if skip_next_empty and line_stripped == '':
            skip_next_empty = False
            continue
        
        skip_next_empty = False
        cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines).strip()

# --- LOGS (HISTÃ“RICO) ---
LOG_FILE = "historico_conversas.json"

def log_conversation(user_msg, assistant_msg, is_safe):
    entry = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "user": user_msg,
        "assistant": assistant_msg,
        "safe": is_safe
    }
    
    try:
        if os.path.exists(LOG_FILE):
            with open(LOG_FILE, "r", encoding="utf-8") as f:
                history = json.load(f)
        else:
            history = []
            
        history.append(entry)
        
        with open(LOG_FILE, "w", encoding="utf-8") as f:
            json.dump(history, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"Erro ao salvar log: {e}")

# --- FUNÃ‡Ã•ES DE ÃUDIO ---
async def text_to_speech(text: str, gender: str = "female") -> str:
    """Gera Ã¡udio neural a partir do texto usando Edge-TTS"""
    clean_text = clean_text_for_tts(text)
    
    if gender == "male":
        voice = "pt-BR-AntonioNeural"
    else:
        voice = "pt-BR-FranciscaNeural"
        
    communicate = edge_tts.Communicate(clean_text, voice)
    filename = f"temp_{uuid.uuid4()}.mp3"
    # Salvar no diretÃ³rio do backend
    filepath = os.path.join(BACKEND_DIR, filename)
    await communicate.save(filepath)
    
    # Aguardar um pouco e verificar se o arquivo foi salvo corretamente
    import asyncio
    await asyncio.sleep(0.1)  # Pequeno delay para garantir que o arquivo estÃ¡ escrito
    
    if not os.path.exists(filepath):
        raise Exception(f"Arquivo nÃ£o foi salvo corretamente: {filepath}")
    
    file_size = os.path.getsize(filepath)
    print(f"ğŸ’¾ Arquivo salvo: {filename}")
    print(f"ğŸ“ Caminho completo: {filepath}")
    print(f"ğŸ“Š Tamanho do arquivo: {file_size} bytes")
    print(f"âœ… Arquivo existe apÃ³s salvar: {os.path.exists(filepath)}")
    
    if file_size == 0:
        raise Exception(f"Arquivo estÃ¡ vazio: {filepath}")
    
    return filename  # Retornar apenas o nome do arquivo para a URL

async def speech_to_text(file_path: str) -> str:
    """Transcreve Ã¡udio usando Groq Whisper"""
    with open(file_path, "rb") as file:
        transcription = groq_client.audio.transcriptions.create(
            file=(file_path, file.read()),
            model="whisper-large-v3",
            language="pt"
        )
    return transcription.text

# --- API ---
class ChatRequest(BaseModel):
    message: str

@app.post("/chat")
async def chat_endpoint(
    message: str = Form(None)
):
    # 1. Validar Input (Apenas Texto - Ãudio serÃ¡ implementado no futuro)
    if not message:
        raise HTTPException(status_code=400, detail="Envie uma mensagem de texto")
    
    user_msg = message

    print(f"ğŸ“ Input UsuÃ¡rio: {user_msg}")

    # 2. ValidaÃ§Ã£o de SeguranÃ§a (Guardrails)
    print("ğŸ”’ Verificando Guardrails...")
    is_safe = check_guardrails(user_msg)
    final_response_text = ""

    if not is_safe:
        print("âš ï¸ Bloqueado por Guardrails")
        final_response_text = "âš ï¸ ALERTA DE SEGURANÃ‡A: NÃ£o posso orientar sobre medicaÃ§Ã£o ou emergÃªncias. Contate o mÃ©dico imediatamente."
    else:
        # 3. RAG + GeraÃ§Ã£o
        try:
            print("ğŸ§  Chamando LLM (RAG)...")
            raw_response = chain.invoke(user_msg)
            # Limpar cabeÃ§alhos indesejados da resposta
            final_response_text = clean_response_text(raw_response)
            print("âœ… Resposta gerada com sucesso.")
        except Exception as e:
            error_msg = str(e)
            error_traceback = traceback.format_exc()
            print(f"âŒ Erro na IA: {error_msg}")
            print(f"ğŸ“‹ Traceback completo:\n{error_traceback}")
            final_response_text = "Desculpe, tive um erro tÃ©cnico ao processar sua solicitaÃ§Ã£o."
            is_safe = False

    # Salvar Log
    log_conversation(user_msg, final_response_text, is_safe)

    # 4. Retornar Resposta (Apenas Texto - Ãudio serÃ¡ implementado no futuro)
    response_data = {
        "text_response": final_response_text,
        "audio_url": None,  # Desabilitado temporariamente
        "is_safe": is_safe
    }
    
    print(f"ğŸ“¤ Resposta enviada (texto apenas)")
    return response_data

@app.get("/audio/{filename}")
async def get_audio(filename: str):
    # Procurar o arquivo no diretÃ³rio do backend
    path = os.path.join(BACKEND_DIR, filename)
    
    print(f"ğŸ” Buscando Ã¡udio: {filename}")
    print(f"ğŸ“ DiretÃ³rio backend: {BACKEND_DIR}")
    print(f"ğŸ“ Caminho completo: {path}")
    print(f"âœ… Arquivo existe: {os.path.exists(path)}")
    
    if os.path.exists(path):
        print(f"âœ… Servindo arquivo: {path}")
        # Servir arquivo com headers CORS explÃ­citos
        response = FileResponse(
            path, 
            media_type="audio/mpeg",
            filename=filename,
            headers={
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "GET",
                "Access-Control-Allow-Headers": "*",
            }
        )
        return response
    else:
        print(f"âŒ Arquivo nÃ£o encontrado: {path}")
        # Debug: listar arquivos MP3 no diretÃ³rio backend
        if os.path.exists(BACKEND_DIR):
            mp3_files = [f for f in os.listdir(BACKEND_DIR) if f.endswith('.mp3')]
            print(f"ğŸ“‹ Arquivos MP3 no diretÃ³rio backend ({BACKEND_DIR}): {mp3_files}")
        raise HTTPException(status_code=404, detail=f"Arquivo nÃ£o encontrado: {filename}")

@app.get("/")
def read_root():
    return {"status": "online", "service": "Agente Cuidador POC"}

@app.get("/health")
def health_check():
    """Endpoint para verificar se o servidor estÃ¡ online"""
    return {
        "status": "online",
        "service": "Agente Cuidador POC",
        "timestamp": datetime.now().isoformat()
    }
